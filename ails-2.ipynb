{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install rdkit-pypi -qqq\nimport rdkit\nfrom rdkit import Chem\nfrom rdkit.Chem import AllChem, DataStructs\nimport rdkit\n!pip install fcd\nimport fcd","metadata":{"id":"qeFZfhnTfRyK","outputId":"9f029523-c3e2-475b-a0be-5cc0d45e4c60","execution":{"iopub.status.busy":"2022-07-31T09:53:18.852737Z","iopub.execute_input":"2022-07-31T09:53:18.853175Z","iopub.status.idle":"2022-07-31T09:54:10.241400Z","shell.execute_reply.started":"2022-07-31T09:53:18.853092Z","shell.execute_reply":"2022-07-31T09:54:10.240371Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"# Neural-based generator of organic compounds\nimport functools\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.nn.functional import one_hot\nfrom tqdm import tqdm\nfrom rdkit import RDLogger\nimport pandas as pd\n\n\n__special__ = {0: \"<PAD>\", 1: \"<BOS>\", 2: \"<EOS>\"}\nRDLogger.DisableLog('rdApp.*')\n\nclass SmilesProvider(torch.utils.data.DataLoader):\n    def __init__(self,file,total=130):\n        self.total = total\n        print(file)\n        self.smiles = open(file, 'r').read().split(\"\\n\")[:-1]\n        tokens = functools.reduce(lambda acc,s: acc.union(set(s)), self.smiles ,set())\n        self.vocsize = len(tokens) + len(__special__)\n        self.index2token = dict(enumerate(tokens,start=3))\n        self.index2token.update(__special__)\n        self.token2index = {v:k for k,v in self.index2token.items()}\n        self.ints = [torch.LongTensor([self.token2index[s] for s in line]) for line in tqdm(self.smiles,\"Preparing of a dataset\")]\n\n    def decode(self,indexes):\n        return \"\".join([self.index2token[index] for index in indexes if index not in __special__])\n\n    def __getitem__(self,i):\n        special_added = torch.cat((torch.LongTensor([self.token2index['<BOS>']])\n                                   ,self.ints[i],torch.LongTensor([self.token2index['<EOS>']]),\n                                   torch.LongTensor([self.token2index[\"<PAD>\"]]*(self.total-len(self.ints[i])-2))),dim=0)\n        return one_hot(special_added,self.vocsize).float(),special_added\n\n    def __len__(self):\n        return len(self.smiles)\n\nclass SimpleGRU(nn.Module):\n\n    def __init__(self, vocsize,device,hidden_size=512,num_layers=3):\n        super().__init__()\n        self.device = device\n        self.vocsize = vocsize\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n        self.gru = nn.GRU(vocsize,hidden_size,bidirectional=False,batch_first=True,num_layers=num_layers)\n        self.linear = nn.Linear(hidden_size,vocsize)\n\n\n    def forward(self,x):\n        output = self.gru(x)[0]\n        final = self.linear(output)\n        return final\n\n    def sample(self,batch_size=128,max_len=130):\n        bos_token = [k for k,v in __special__.items() if v == \"<BOS>\"][0]\n        x = torch.LongTensor([bos_token]*batch_size)\n        h = torch.zeros((self.num_layers,batch_size,self.hidden_size)).to(self.device)\n        accumulator = torch.zeros(batch_size,max_len)\n        for i in range(max_len):\n            x = one_hot(x, self.vocsize).float().unsqueeze(1).to(self.device)\n            output,h = self.gru(x,h)\n            next = F.softmax(self.linear(output).squeeze(1),dim=1)\n            x = torch.multinomial(next,num_samples=1,replacement=True).squeeze(1)\n            accumulator[:,i] = x\n        return accumulator\n\ndef generate(file='../input/genmodel1epoch/genmodel-1Epoch.pt',batch_size=1000):\n    \"\"\"\n    This is the entrypoint for the generator of SMILES\n    :param file: A file with pretrained model\n    :param batch_size: The number of compounds to generate\n    :return: None. It prints a list of generated compounds to stdout\n    \"\"\"\n    box= torch.load(file)\n    model,tokenizer = box['model'],box['tokenizer']\n    model.eval()\n    res = model.sample(batch_size)\n    \n    correct =0\n    smiles_list = []\n    for i in range(res.size(0)):\n        smiles = \"\".join([tokenizer[index] for index in res[i].tolist() if index not in __special__])\n        correct = correct + 1 if Chem.MolFromSmiles(smiles) else correct\n        smiles_list.append(smiles)\n    \n    smiles_pd = pd.DataFrame(smiles_list)\n    smiles_pd.to_csv('res15.txt', sep='\\n', encoding='utf-8',index = False,header = False)\n    print (\"% of correct molecules is {:4.2f}\".format(correct/float(batch_size)*100))\n\ndef train(file='../input/smiles-train/smiles_train.txt',batch_size=256,learning_rate=0.001,n_epochs=5,device='cuda'):\n    \"\"\"\n    This is the entrypoint for training of the RNN\n    :param file: A file with molecules in SMILES notation\n    :param batch_size: A batch size for training\n    :param learning_rate: A learning rate of the optimizer\n    :param n_epochs: A number of epochs\n    :param device: \"cuda\" for GPU training, \"cpu\" for training on CPU, if there are no CUDA on a computer it uses CPU only\n    :return: None. It saves the model to \"genmodel.pt\" file\n    \"\"\"\n    print(file)\n    device = device if torch.cuda.is_available() else 'cpu'\n    dataset = SmilesProvider(Chem.MolFromSmiles(file))\n    model = SimpleGRU(dataset.vocsize,device=device).to(device)\n    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n    loss_function = nn.CrossEntropyLoss()\n    model.train()\n    for epoch in range(1, n_epochs + 1):\n        for iteration,(batch,target) in enumerate(tqdm(dataloader,'Training')):\n            batch,target = batch.to(device),target.to(device)\n            out = model(batch)\n            out = out.transpose(2,1)\n            loss = loss_function(out[:,:,:-1],target[:,1:])\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n    model.device = 'cpu'\n    torch.save({'tokenizer':dataset.index2token,'model':model.cpu()},\"genmodel_5Epoch.pt\")\n    \n\n","metadata":{"id":"ZUORUU9we_S8","outputId":"3720b996-0510-475d-c51a-320d2945a39e","execution":{"iopub.status.busy":"2022-07-31T09:55:52.725534Z","iopub.execute_input":"2022-07-31T09:55:52.725963Z","iopub.status.idle":"2022-07-31T09:55:52.780774Z","shell.execute_reply.started":"2022-07-31T09:55:52.725914Z","shell.execute_reply":"2022-07-31T09:55:52.775384Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"train()\n","metadata":{"id":"S14--BXyfqNg","outputId":"3f3d59fc-99a5-4f41-8811-c39c75c97ac5","execution":{"iopub.status.busy":"2022-07-31T09:55:54.785872Z","iopub.execute_input":"2022-07-31T09:55:54.786464Z","iopub.status.idle":"2022-07-31T09:55:54.834584Z","shell.execute_reply.started":"2022-07-31T09:55:54.786429Z","shell.execute_reply":"2022-07-31T09:55:54.833272Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"generate()","metadata":{"id":"e30CxJ6_hJez","execution":{"iopub.status.busy":"2022-07-31T08:42:19.081231Z","iopub.execute_input":"2022-07-31T08:42:19.081993Z","iopub.status.idle":"2022-07-31T08:42:33.944499Z","shell.execute_reply.started":"2022-07-31T08:42:19.081951Z","shell.execute_reply":"2022-07-31T08:42:33.943509Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"filenames = ['../input/result-faulty/res_final.txt','res11.txt','res12.txt','res13.txt','res14.txt','res15.txt']\nwith open('res_final_2.txt', 'w') as outfile:\n    for fname in filenames:\n        with open(fname) as infile:\n            outfile.write(infile.read())","metadata":{"execution":{"iopub.status.busy":"2022-07-31T08:47:41.195148Z","iopub.execute_input":"2022-07-31T08:47:41.195843Z","iopub.status.idle":"2022-07-31T08:47:41.206908Z","shell.execute_reply.started":"2022-07-31T08:47:41.195802Z","shell.execute_reply":"2022-07-31T08:47:41.205791Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"\nimport os\nimport pkgutil\nimport tempfile\nfrom rdkit import Chem\nfrom multiprocessing import Pool\nimport fcd\nimport numpy as np\n\n\ndef loadmodel():\n    chemnet_model_filename = 'ChemNet_v0.13_pretrained.h5'\n    model_bytes = pkgutil.get_data('fcd', chemnet_model_filename)\n\n    with tempfile.TemporaryDirectory() as tmpdir:\n        model_path = os.path.join(tmpdir, 'chemnet.h5')\n\n        with open(model_path, 'wb') as f:\n            f.write(model_bytes)\n\n        print(f'Saved ChemNet model to \\'{model_path}\\'')\n\n        return fcd.load_ref_model(model_path)\n\n\ndef getstats(smiles, model):\n    predictions = fcd.get_predictions(model, smiles)\n    mean = predictions.mean(0)\n    cov = np.cov(predictions.T)\n    return mean, cov\n\ndef _cansmi(smi):\n    \"\"\"Try except is needed in case rdkit throws an error\"\"\"\n    try:\n        mol = Chem.MolFromSmiles(smi, sanitize=True)\n        can_smi = Chem.MolToSmiles(mol)\n    except:\n        can_smi = None\n    return can_smi\n\n\ndef canonicalize_smiles(smiles, njobs=8):\n    r\"\"\"calculates canonical smiles\n    Arguments:\n        smiles (list): List of smiles\n        njobs (int): How many workers to use\n\n    Returns:\n        canonical_smiles: A list of canonical smiles. None if invalid smiles.\n    \"\"\"\n\n    with Pool(njobs) as pool:\n        # pairs of mols and canonical smiles\n        canonical_smiles = pool.map(_cansmi, smiles)\n\n    return canonical_smiles\n\nwith open('res_final_2.txt') as f:\n        smiles_gen = [s for s in f.read().split() if s][:15000]\nsmiles_can = canonicalize_smiles(smiles_gen)\nsmiles_valid = [s for s in smiles_can if s is not None]\nsmiles_unique = set(smiles_valid)\n\nvalidity = len(smiles_valid) / len(smiles_gen)\nuniqueness = len(smiles_unique) / len(smiles_gen)\n#m = Chem.MolFromSmiles(smiles_gen)\nsmiles_pd = pd.DataFrame(smiles_valid)\nsmiles_pd.to_csv('res_3.txt', sep='\\n', encoding='utf-8',index = False,header = False)\n","metadata":{"execution":{"iopub.status.busy":"2022-07-31T09:36:56.656118Z","iopub.execute_input":"2022-07-31T09:36:56.656651Z","iopub.status.idle":"2022-07-31T09:37:00.945865Z","shell.execute_reply.started":"2022-07-31T09:36:56.656612Z","shell.execute_reply":"2022-07-31T09:37:00.944513Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"code","source":"","metadata":{"jupyter":{"source_hidden":true}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"open('../input/smiles-train/smiles_train.txt', 'r').read().split(\"\\n\")[:-1]","metadata":{"execution":{"iopub.status.busy":"2022-07-31T09:48:09.102830Z","iopub.execute_input":"2022-07-31T09:48:09.103241Z","iopub.status.idle":"2022-07-31T09:48:10.971621Z","shell.execute_reply.started":"2022-07-31T09:48:09.103206Z","shell.execute_reply":"2022-07-31T09:48:10.969127Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}